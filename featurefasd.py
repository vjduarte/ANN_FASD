# -*- coding: utf-8 -*-
"""FeatureFASD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ODW00m6tYj155NFnWpEX08smEhLR_epX
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import feature_column
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split
from keras.layers import LeakyReLU

AccuGra=[]
psyco=pd.read_excel('Data.xlsx', sheet_name='Psyco')
ProSac=pd.read_excel('Data.xlsx', sheet_name='ProSac')
MGSac=pd.read_excel('Data.xlsx', sheet_name='MGSac')
DTI=pd.read_excel('Data.xlsx', sheet_name='DTI')
AntiSac=pd.read_excel('Data.xlsx', sheet_name='AntiSac')

#Arreglo el sexo para que sea 0 o 1
def sex_fix(s):
    if s=='M' or s=='Male':
        return 0
    return 1


def group_fix(s):
  if s == 1 :
    return 0
  return 1

AntiSac['sex']=AntiSac.apply(lambda x: sex_fix(x['sex']), axis=1);
AntiSac.head()
AntiSac['group']=AntiSac.apply(lambda x: group_fix(x['group']), axis=1)

ProSac['sex']=ProSac.apply(lambda x: sex_fix(x['sex']), axis=1);
ProSac['group']=ProSac.apply(lambda x: group_fix(x['group']), axis=1)
ProSac.head()

MGSac['sex']=MGSac.apply(lambda x: sex_fix(x['sex']), axis=1);
MGSac.head()
MGSac['group']=MGSac.apply(lambda x: group_fix(x['group']), axis=1)

x = AntiSac.drop(['Subject',	'type'], axis=1)
df_AS=x.iloc[:, :18]

x = ProSac.drop(['Subject',	'type'], axis=1)
df_PS=x.iloc[:, :21]
df_PS

x = MGSac.drop(['Subject',	'type'], axis=1)
df_MG=x.iloc[:, :29]
df_MG

x = DTI.drop(['Subject',	'type'], axis=1)
df_DTI=x.iloc[:, :51]
df_DTI

x = psyco.drop(['Subject',	'type'], axis=1)
df_Psy=x.iloc[:, :51]
df_Psy

"""# **AntiSacadico**"""

train, test = train_test_split(df_AS, test_size=0.2)
train, val = train_test_split(train, test_size=0.2)
print(len(train), 'train examples')
print(len(val), 'validation examples')
print(len(test), 'test examples')

# A utility method to create a tf.data dataset from a Pandas Dataframe
def df_to_dataset(dataframe, shuffle=True, batch_size=32):
  dataframe = dataframe.copy()
  labels = dataframe.pop('group')
  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
  if shuffle:
    ds = ds.shuffle(buffer_size=len(dataframe))
  ds = ds.batch(batch_size)
  return ds

batch_size = 5 # A small batch sized is used for demonstration purposes
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)

for feature_batch, label_batch in train_ds.take(1):
  print('Every feature:', list(feature_batch.keys()))
  print('A batch of ages:', feature_batch['age'])
  print('A batch of targets:', label_batch )

# We will use this batch to demonstrate several types of feature columns
example_batch = next(iter(train_ds))[0]

# A utility method to create a feature column
# and to transform a batch of data
def demo(feature_column):
  feature_layer = layers.DenseFeatures(feature_column)
  #print(feature_layer(example_batch).numpy())

ct = feature_column.numeric_column('ct')
demo(ct)
srt_ct = feature_column.numeric_column('srt_ct')
demo(srt_ct)

CV_CT= feature_column.numeric_column('CV_CT')
demo(CV_CT)
ES_BD = feature_column.numeric_column('ES_BD')
demo(ES_BD)
ES = feature_column.numeric_column('ES')
demo(ES)
STR_ERR = feature_column.numeric_column('STR_ERR')
demo(STR_ERR)
D_ERR= feature_column.numeric_column('D_ERR')
demo(D_ERR)
STR_CS = feature_column.numeric_column('STR_CS')
demo(STR_CS)
CD_ERR = feature_column.numeric_column('CD_ERR')
demo(CD_ERR)

A_ERR= feature_column.numeric_column('A_ERR')
demo(A_ERR)

T_SS = feature_column.numeric_column('T_SS')
demo(T_SS)

vel = feature_column.numeric_column('vel')
demo(vel)

am = feature_column.numeric_column('am')
demo(am)
Ang_1S= feature_column.numeric_column('Ang_1S')
demo(Ang_1S)

SD_SRT = feature_column.numeric_column('sd_srt')
demo(SD_SRT)

age = feature_column.numeric_column('age')
demo(age)

sex = feature_column.numeric_column('sex')
demo(sex)

"""El Algoritmo de Machine Learning

"""

feature_columns = []

# numeric cols
for header in ['sex','age','ct','sd_srt','srt_ct', 'CV_CT', 'ES_BD', 'ES', 'STR_ERR', 'D_ERR', 'STR_CS', 'CD_ERR', 'A_ERR', 'T_SS', 'vel', 'am', 'Ang_1S']:
  feature_columns.append(feature_column.numeric_column(header))

feature_layer = tf.keras.layers.DenseFeatures(feature_columns)

batch_size = 32
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)
train_ds

model = tf.keras.Sequential([
  feature_layer,
  layers.Dense(128, activation='relu'),
  layers.Dense(128, activation='relu'),
  layers.Dropout(.1),
  layers.Dense(1)
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])
history =model.fit(train_ds,validation_data=val_ds,epochs=150)

loss, accuracy = model.evaluate(test_ds)
AccuGra.append(accuracy)
print("Accuracy AntiSac", accuracy)
#***********************************************
#***********************************************
#***********************************************
lab=test['group']
ynew = model.predict_classes(test_ds)
print(lab,ynew)





# serialize model to YAML
model_AnS = model.to_yaml()
with open("modelAnS.yaml", "w") as yaml_file:
    yaml_file.write(model_AnS)
# serialize weights to HDF5
model.save_weights("modelAns.h5")
print("Saved model to disk")


"""# **ProSacadico**"""

train, test = train_test_split(df_PS, test_size=0.2)
train, val = train_test_split(train, test_size=0.2)
print(len(train), 'train examples')
print(len(val), 'validation examples')
print(len(test), 'test examples')

batch_size = 5 # A small batch sized is used for demonstration purposes
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)

for feature_batch, label_batch in train_ds.take(1):
  print('Every feature:', list(feature_batch.keys()))
  print('A batch of ages:', feature_batch['age'])
  print('A batch of targets:', label_batch )

  # We will use this batch to demonstrate several types of feature columns
example_batch = next(iter(train_ds))[0]

# A utility method to create a feature column
# and to transform a batch of data
def demo(feature_column):
  feature_layer = layers.DenseFeatures(feature_column)
  #print(feature_layer(example_batch).numpy())

age = feature_column.numeric_column('age')
demo(age)

sex = feature_column.numeric_column('sex')
demo(sex)

ct = feature_column.numeric_column('ct')
demo(ct)

srt_ct = feature_column.numeric_column('SRT_CT')
demo(srt_ct)

SD_ERR = feature_column.numeric_column('sd_srt')
demo(SD_ERR)

CV_CT= feature_column.numeric_column('c_ct')
demo(CV_CT)


ES_BD = feature_column.numeric_column('es_bd')
demo(ES_BD)

ES = feature_column.numeric_column('es')
demo(ES)

D_ERR= feature_column.numeric_column('d_err')
demo(D_ERR)


A_ERR= feature_column.numeric_column('a_err')
demo(A_ERR)

T_SS = feature_column.numeric_column('t_ss')
demo(T_SS)

vel = feature_column.numeric_column('vel')
demo(vel)

am = feature_column.numeric_column('amp')
demo(am)

Ang_1S= feature_column.numeric_column('ang_1s')
demo(Ang_1S)

dur= feature_column.numeric_column('dur')
demo(dur)

des= feature_column.numeric_column('des')
demo(des)

acc= feature_column.numeric_column('acc')
demo(acc)

sk1= feature_column.numeric_column('sk1')
demo(sk1)

sk2= feature_column.numeric_column('sk2')
demo(sk2)

sk_i= feature_column.numeric_column('sk_i')
demo(sk_i)

feature_columns = []

# numeric cols
for header in ['age', 'sex', 'ct', 'SRT_CT', 'sd_srt', 'c_ct', 'es', 'es_bd', 'd_err', 'a_err', 't_ss', 'vel', 'amp', 'ang_1s', 'dur', 'des', 'acc', 'sk1', 'sk2', 'sk_i']:
  feature_columns.append(feature_column.numeric_column(header))

feature_layer = tf.keras.layers.DenseFeatures(feature_columns)

batch_size = 18
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)
train_ds

model = tf.keras.Sequential([
  feature_layer,
  layers.Dense(128, activation='relu'),
  layers.Dense(128, activation='relu'),
  layers.Dropout(.1),
  layers.Dense(1)
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])
model.fit(train_ds,validation_data=val_ds,epochs=10)

loss, accuracy = model.evaluate(test_ds)
AccuGra.append(accuracy)
print("Accuracy: -ProSac-->", accuracy)

#***********************************************
#***********************************************
#***********************************************
lab=test['group']
ynew = model.predict_classes(test_ds)
print(lab,ynew)


# serialize model to YAML
model_Pro = model.to_yaml()
with open("modelPro.yaml", "w") as yaml_file:
    yaml_file.write(model_Pro)
# serialize weights to HDF5
model.save_weights("modelPro.h5")
print("Saved model to disk")

"""# **MGSacadic**"""

train, test = train_test_split(df_MG, test_size=0.2)
train, val = train_test_split(train, test_size=0.2)
print(len(train), 'train examples')
print(len(val), 'validation examples')
print(len(test), 'test examples')

# A utility method to create a tf.data dataset from a Pandas Dataframe
def df_to_dataset(dataframe, shuffle=True, batch_size=32):
  dataframe = dataframe.copy()
  labels = dataframe.pop('group')
  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
  if shuffle:
    ds = ds.shuffle(buffer_size=len(dataframe))
  ds = ds.batch(batch_size)
  return ds

batch_size = 5 # A small batch sized is used for demonstration purposes
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)

for feature_batch, label_batch in train_ds.take(1):
  print('Every feature:', list(feature_batch.keys()))
  print('A batch of ages:', feature_batch['age'])
  print('A batch of targets:', label_batch )

# We will use this batch to demonstrate several types of feature columns
example_batch = next(iter(train_ds))[0]

# A utility method to create a feature column
# and to transform a batch of data
def demo(feature_column):
  feature_layer = layers.DenseFeatures(feature_column)
  #print(feature_layer(example_batch).numpy())

age = feature_column.numeric_column('age')
demo(age)

sex = feature_column.numeric_column('sex')
demo(sex)

ct = feature_column.numeric_column('ct')
demo(ct)

srt_ct = feature_column.numeric_column('SRT_CT')
demo(srt_ct)

SD_SRT = feature_column.numeric_column('sd_srt')
demo(SD_SRT)

CV_CT= feature_column.numeric_column('c_ct')
demo(CV_CT)


srt_ct2 = feature_column.numeric_column('srt_ct2')
demo(srt_ct2)

SD_SRT2 = feature_column.numeric_column('sd_srt2')
demo(SD_SRT2)

CV_CT2= feature_column.numeric_column('c_ct2')
demo(CV_CT2)

ES = feature_column.numeric_column('es')
demo(ES)

STR_ERR = feature_column.numeric_column('t_s1_s2')
demo(STR_ERR)

D_ERR= feature_column.numeric_column('t_s1')
demo(D_ERR)

STR_CS = feature_column.numeric_column('t_te')
demo(STR_CS)
CD_ERR = feature_column.numeric_column('t_err')
demo(CD_ERR)

A_ERR= feature_column.numeric_column('fs_se')
demo(A_ERR)

fs = feature_column.numeric_column('fs')
demo(fs)

at_err = feature_column.numeric_column('at_err')
demo(at_err)

s_err = feature_column.numeric_column('s_err')
demo(s_err)

t_ss= feature_column.numeric_column('t_ss')
demo(t_ss)

vel = feature_column.numeric_column('vel')
demo(vel)

am = feature_column.numeric_column('amp')
demo(am)

vel2 = feature_column.numeric_column('vel2')
demo(vel2)

am2 = feature_column.numeric_column('amp2')
demo(am2)

Ang_1S= feature_column.numeric_column('ang_1s')
demo(Ang_1S)

a_1s = feature_column.numeric_column('a_1s')
demo(a_1s)

af2 = feature_column.numeric_column('a_f2')
demo(af2)

acf = feature_column.numeric_column('a_cf')
demo(acf)

pal= feature_column.numeric_column('p_al')
demo(pal)

feature_columns = []

# numeric cols
for header in ['age', 'sex', 'ct', 'SRT_CT', 'sd_srt', 'c_ct', 'srt_ct2', 'sd_srt2', 'c_ct2', 'es', 't_s1_s2', 't_s1', 't_te', 't_err', 'fs_se', 'fs', 'at_err', 's_err', 't_ss', 'vel', 'amp', 'vel2', 'amp2', 'ang_1s', 'a_1s', 'a_f2', 'a_cf', 'p_al']:
  feature_columns.append(feature_column.numeric_column(header))

feature_layer = tf.keras.layers.DenseFeatures(feature_columns)

batch_size = 18
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)
train_ds

model = tf.keras.Sequential([
  feature_layer,
  layers.Dense(64, activation='relu'),
  layers.Dense(128, activation='sigmoid'),
  layers.Dense(64, activation='sigmoid'),
  layers.Dense(128, activation='relu'),
  layers.Dropout(.1),
  layers.Dense(1)
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])
model.fit(train_ds,validation_data=val_ds,epochs=10)

loss, accuracy = model.evaluate(test_ds)
AccuGra.append(accuracy)
print("Accuracy: MGSac--->", accuracy)

#***********************************************
#***********************************************
#***********************************************
lab=test['group']
ynew = model.predict_classes(test_ds)
print(lab,ynew)

# serialize model to YAML
model_MGS= model.to_yaml()
with open("modelMGS.yaml", "w") as yaml_file:
    yaml_file.write(model_MGS)
# serialize weights to HDF5
model.save_weights("modelMGS.h5")
print("Saved model to disk")

"""# **DTI**"""

train, test = train_test_split(df_DTI, test_size=0.2)
train, val = train_test_split(train, test_size=0.2)
print(len(train), 'train examples')
print(len(val), 'validation examples')
print(len(test), 'test examples')

# A utility method to create a tf.data dataset from a Pandas Dataframe
def df_to_dataset(dataframe, shuffle=True, batch_size=32):
  dataframe = dataframe.copy()
  labels = dataframe.pop('group')
  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
  if shuffle:
    ds = ds.shuffle(buffer_size=len(dataframe))
  ds = ds.batch(batch_size)
  return ds

batch_size = 5 # A small batch sized is used for demonstration purposes
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)

for feature_batch, label_batch in train_ds.take(1):
  print('Every feature:', list(feature_batch.keys()))
  #print('A batch of ages:', feature_batch['age'])
  print('A batch of targets:', label_batch )

# We will use this batch to demonstrate several types of feature columns
example_batch = next(iter(train_ds))[0]

# A utility method to create a feature column
# and to transform a batch of data
def demo(feature_column):
  feature_layer = layers.DenseFeatures(feature_column)
  #print(feature_layer(example_batch).numpy())

f1 = feature_column.numeric_column('f1')
demo(f1)

f2 = feature_column.numeric_column('f2')
demo(f2)

f3 = feature_column.numeric_column('f3')
demo(f3)

f4 = feature_column.numeric_column('f4')
demo(f4)

f5 = feature_column.numeric_column('f5')
demo(f5)

f6 = feature_column.numeric_column('f6')
demo(f6)

f7 = feature_column.numeric_column('f7')
demo(f7)

f8 = feature_column.numeric_column('f8')
demo(f8)

f9 = feature_column.numeric_column('f9')
demo(f9)

f10 = feature_column.numeric_column('f10')
demo(f10)

f11 = feature_column.numeric_column('f11')
demo(f11)

f12 = feature_column.numeric_column('f12')
demo(f12)

f13 = feature_column.numeric_column('f13')
demo(f13)

f14 = feature_column.numeric_column('f14')
demo(f14)

f15 = feature_column.numeric_column('f15')
demo(f15)

f16 = feature_column.numeric_column('f16')
demo(f16)

f17 = feature_column.numeric_column('f17')
demo(f17)

f18 = feature_column.numeric_column('f18')
demo(f18)

f19 = feature_column.numeric_column('f19')
demo(f19)

f20 = feature_column.numeric_column('f20')
demo(f20)

f21 = feature_column.numeric_column('f21')
demo(f21)

f22 = feature_column.numeric_column('f22')
demo(f22)

f23 = feature_column.numeric_column('f23')
demo(f23)

f24 = feature_column.numeric_column('f24')
demo(f24)

f25 = feature_column.numeric_column('f25')
demo(f25)

f26 = feature_column.numeric_column('f26')
demo(f26)

f27 = feature_column.numeric_column('f27')
demo(f27)

f28 = feature_column.numeric_column('f28')
demo(f28)

f29 = feature_column.numeric_column('f29')
demo(f29)

f30 = feature_column.numeric_column('f30')
demo(f30)

f31 = feature_column.numeric_column('f31')
demo(f31)

f32 = feature_column.numeric_column('f32')
demo(f32)

f33 = feature_column.numeric_column('f33')
demo(f33)

f34 = feature_column.numeric_column('f34')
demo(f34)

f35 = feature_column.numeric_column('f35')
demo(f35)

f36 = feature_column.numeric_column('f36')
demo(f36)

f37 = feature_column.numeric_column('f37')
demo(f37)

f38 = feature_column.numeric_column('f38')
demo(f38)

f39 = feature_column.numeric_column('f39')
demo(f39)

f40 = feature_column.numeric_column('f40')
demo(f40)

f41 = feature_column.numeric_column('f41')
demo(f41)

f42 = feature_column.numeric_column('f42')
demo(f42)

f43 = feature_column.numeric_column('f43')
demo(f43)

f44 = feature_column.numeric_column('f44')
demo(f44)

f45 = feature_column.numeric_column('f45')
demo(f45)

f46 = feature_column.numeric_column('f46')
demo(f46)

f47 = feature_column.numeric_column('f47')
demo(f47)

f48 = feature_column.numeric_column('f48')
demo(f48)

feature_columns = []

# numeric cols
for header in ['f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48']:
  feature_columns.append(feature_column.numeric_column(header))

feature_layer = tf.keras.layers.DenseFeatures(feature_columns)

batch_size = 18
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)
train_ds

model = tf.keras.Sequential([
  feature_layer,
  layers.Dense(64, activation='relu'),
  layers.Dense(128, activation='sigmoid'),
  layers.Dense(64, activation='sigmoid'),
  layers.Dense(128, activation='relu'),
  layers.Dropout(.1),
  layers.Dense(1)
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])
model.fit(train_ds,validation_data=val_ds,epochs=10)

loss, accuracy = model.evaluate(test_ds)
AccuGra.append(accuracy)
print("Accuracy: DTI--->", accuracy)

#***********************************************
#***********************************************
#***********************************************
lab=test['group']
ynew = model.predict_classes(test_ds)
print(lab,ynew)

# serialize model to YAML
model_DTI = model.to_yaml()
with open("modelDTI.yaml", "w") as yaml_file:
    yaml_file.write(model_DTI)
# serialize weights to HDF5
model.save_weights("modelDTI.h5")
print("Saved model to disk")


"""# **Pysco**"""

train, test = train_test_split(df_Psy, test_size=0.2)
train, val = train_test_split(train, test_size=0.2)
print(len(train), 'train examples')
print(len(val), 'validation examples')
print(len(test), 'test examples')

# A utility method to create a tf.data dataset from a Pandas Dataframe
def df_to_dataset(dataframe, shuffle=True, batch_size=32):
  dataframe = dataframe.copy()
  labels = dataframe.pop('group')
  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
  if shuffle:
    ds = ds.shuffle(buffer_size=len(dataframe))
  ds = ds.batch(batch_size)
  return ds

batch_size = 5 # A small batch sized is used for demonstration purposes
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)

for feature_batch, label_batch in train_ds.take(1):
  print('Every feature:', list(feature_batch.keys()))
  #print('A batch of ages:', feature_batch['age'])
  print('A batch of targets:', label_batch )

# We will use this batch to demonstrate several types of feature columns
example_batch = next(iter(train_ds))[0]

# A utility method to create a feature column
# and to transform a batch of data
def demo(feature_column):
  feature_layer = layers.DenseFeatures(feature_column)
  #print(feature_layer(example_batch).numpy())

f1 = feature_column.numeric_column('f1')
demo(f1)

f2 = feature_column.numeric_column('f2')
demo(f2)

f3 = feature_column.numeric_column('f3')
demo(f3)

f4 = feature_column.numeric_column('f4')
demo(f4)

f5 = feature_column.numeric_column('f5')
demo(f5)

f6 = feature_column.numeric_column('f6')
demo(f6)

f7 = feature_column.numeric_column('f7')
demo(f7)

f8 = feature_column.numeric_column('f8')
demo(f8)

f9 = feature_column.numeric_column('f9')
demo(f9)

f10 = feature_column.numeric_column('f10')
demo(f10)

f11 = feature_column.numeric_column('f11')
demo(f11)

f12 = feature_column.numeric_column('f12')
demo(f12)

f13 = feature_column.numeric_column('f13')
demo(f13)

f14 = feature_column.numeric_column('f14')
demo(f14)

f15 = feature_column.numeric_column('f15')
demo(f15)

f16 = feature_column.numeric_column('f16')
demo(f16)

f17 = feature_column.numeric_column('f17')
demo(f17)

f18 = feature_column.numeric_column('f18')
demo(f18)

f19 = feature_column.numeric_column('f19')
demo(f19)

f20 = feature_column.numeric_column('f20')
demo(f20)

feature_columns = []

# numeric cols
for header in ['f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20']:
  feature_columns.append(feature_column.numeric_column(header))

feature_layer = tf.keras.layers.DenseFeatures(feature_columns)

batch_size = 18
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)
train_ds

model = tf.keras.Sequential([
  feature_layer,
  layers.Dense(64, activation='relu'),
  layers.Dense(128, activation='sigmoid'),
  layers.Dense(64, activation='sigmoid'),
  layers.Dense(128, activation='relu'),
  layers.Dropout(.1),
  layers.Dense(1)
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])
model.fit(train_ds,validation_data=val_ds,epochs=50)

loss, accuracy = model.evaluate(test_ds)
print("Accuracy: Psyco-->", accuracy)
AccuGra.append(accuracy)

#***********************************************
#***********************************************
#***********************************************
lab=test['group']
ynew = model.predict_classes(test_ds)
print(lab,ynew)

# serialize model to JSON
model_json = model.to_json()
with open("model.json", "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model.save_weights("model.h5")
print("Saved model to disk")
Res=[74,78,77,68,84]
#GRAFICAR LOS  RESULTADOS
Estudio=['AntiSac','ProSac','MGSac','DTI','Psyco']
fig, ax = plt.subplots()
#Colocamos una etiqueta en el eje Y
ax.set_ylabel('Accuracy of the model')
plt.grid(True)
#Colocamos una etiqueta en el eje X
ax.set_title('Model Performance')
#Creamos la grafica de barras utilizando 'paises' como eje X y 'ventas' como eje y.
plt.bar(Estudio, AccuGra)
plt.savefig('SimpleCombination.png')
#Finalmente mostramos la grafica con el metodo show()
plt.show()

"""# **Join data**"""

x = AntiSac.drop(['type'], axis=1)
df_AS=x.iloc[:, :18]

x = ProSac.drop(['type'], axis=1)
df_PS=x.iloc[:, :21]
df_PS

x = MGSac.drop(['type'], axis=1)
df_MG=x.iloc[:, :29]
df_MG

x = DTI.drop(['type'], axis=1)
df_DTI=x.iloc[:, :52]
df_DTI

x = psyco.drop(['type'], axis=1)
df_Psy=x.iloc[:, :22]
df_Psy

AntiSac_ProSac=df_PS.merge(df_AS,on='Subject')
AntiSac_ProSac

AntiSac_MGSac=df_MG.merge(df_AS,on='Subject')
AntiSac_MGSac

AntiSac_DTI=df_DTI.merge(df_AS,on='Subject')
AntiSac_DTI

AntiSac_pyso=df_Psy.merge(df_AS,on='Subject')
AntiSac_pyso

ProSac_MGSac=df_MG.merge(df_PS,on='Subject')
ProSac_MGSac

ProSac_DTI=df_DTI.merge(df_PS,on='Subject')
ProSac_DTI

ProSac_pyso=psyco.merge(df_PS,on='Subject')
ProSac_pyso

MGSac_DTI=df_DTI.merge(df_MG,on='Subject')
MGSac_DTI

MGSac_pyso=df_Psy.merge(df_MG,on='Subject')
MGSac_pyso

DTI_pyso=df_Psy.merge(df_DTI,on='Subject')
DTI_pyso

